
"只要你关注" "只要你需要关注"





Illia Polosukhin vidia.polosukhin@gmail.com 电话:(212)963-917


摘要摘要


主要的序列转换模型基于复杂的重复或进化神经网络,其中包括编码器和解码器。最有效果的模型还将编码器和解码器通过关注机制连接起来。我们建议了一个新的简单的网络结构,即完全基于关注机制的变异器,完全通过重复和卷变来进行。在两个机器翻译任务上进行的实验显示,这些模型质量优,同时更可平行,培训时间要少得多。我们的模型在WMT 2014 英文到德文翻译任务上实现了28.4 BLEU,改进了现有的最佳结果,包括2 BLEU的组合。在WMT 2014 英文到法文翻译任务上,我们的模型在八个GPUs培训了3.5天之后确定了新的BLEU最新分数41.8,这是从文献中最佳模型培训成本的一小部分。我们显示,变异器通过将它成功地应用于英语组群,用大而有限的培训数据进行分类,对其他任务作了概括。


一. 导言 1


经常神经网络,长期短期记忆[13]和封闭的经常[7]神经网络,特别是已牢固地确立为在诸如语言建模和机器翻译[35、2、5]等序列建模和转换问题方面最先进的做法,此后继续作出大量努力,以拉近经常性语言模型和编码器-编码器结构的界限[38、24、15]。


经常模式通常按照输入和输出序列的符号位置进行计算。 将位置与计算时间的阶梯对齐,它们产生一系列隐藏状态$ht}美元,作为先前隐藏状态$ht-1}美元和职位美元投入的函数。 这种内在的相继性质排除了培训实例的平行化,培训实例在较长的序列长度上变得至关重要,因为记忆限制限制了对各个示例的分批。最近的工作通过乘数化技巧[21]和有条件计算[32]在计算效率方面取得了显著的提高,同时也改进了后者的模型性能。然而,连续计算的基本限制依然存在。


注意机制已成为各种任务强制顺序建模和转换模型的一个组成部分,允许在不考虑投入或产出序列[2、19] 距离的情况下对依赖性建模,但除少数情况外,[27] 都与经常网络结合使用这种关注机制。


在这项工作中,我们提出了“变换器 ” ( 变换器 ), 模型架构避免重现,而完全依靠关注机制来吸引输入和输出之间的全球依赖性。 变换器可以大大地实现平行化,并且可以在8个P100GPU系统上接受不到12小时的培训后达到翻译质量的新水平。


背景2 背景


减少连续计算的目标也构成了扩展神经GPU [16]、ByteNet [18] 和ConvS2S [9] 的基础,所有这些系统都使用进化神经网络作为基本构件,同时计算所有输入和输出位置的隐藏显示器。在这些模型中,将两个任意输入或输出位置的信号联系起来所需的操作数量在位置之间的距离上增长,ConvS2S和ByteNet的线性增长。这使得更难了解遥远位置之间的依赖性[12]。在变异器中,这被减少到一个不变的操作数量,尽管由于平均关注加权位置而降低有效分辨率的代价,正如第3.2节所描述的那样,这种效果我们抵制多领导人的关注。


自我注意,有时被称为 " 内部注意 " 是一种关注机制,它涉及一个序列的不同位置,以便计算顺序的表示,自我注意成功地用于各种任务,包括阅读理解、抽象总结、文字要求和学习任务独立的句子陈述[4、27、28、22]。


端对端记忆网络以经常性关注机制为基础,而不是按顺序重现,已证明在简单语言回答问题和语言模拟任务方面表现良好[34]。


然而,据我们所知,变换器是第一种完全依靠自觉计算其输入和输出的输入和输出的转换模型,而不使用顺序对齐的RNN或变速。 在以下各节,我们将描述变换器,激发自我注意,并讨论其相对于[17、18]和[9]等模型的优势。


3 建模建模架构


大多数有竞争力的神经序列转换模型有一个编码器-解码器结构 [5, 2, 35] 。 这里, 编码器将一个符号代表值$( x1},......, xn}) 的输入序列绘制成一个连续表示值 $( mathbf{z} ; = ; (z1},...,..., zn} 美元) 的序列。 考虑到$\ mathbf{z} 美元, 解码器然后生成一个符号的输出序列 $\ left( \\\\\\\\\\\\\\\\\ 1},... y\ right) $( $) 。 每一步该模型都是自动递增 [10], 在生成下一个符号时消耗先前生成的符号作为额外输入 。


图1:变形器-模型结构。


变形器遵循这一总体结构,对编码器和解码器都使用堆叠的自我注意和点针,完全连接的层层,分别列于图1的左右两半。


3.1 编码器和编码器堆叠


编码器 : 编码器由堆叠的 $N = 6$ 相同的层组成 。 每层有两个子层 。 第一个是多头自控机制, 第二个是简单、 位置上完全连接的 feed- forward 网络 。 我们使用两个子层周围的剩余连接 [11], 之后是层正常化 [1] 。 也就是说, 每个子层的输出值是层Nom $ (xmathrm{Sublayer}(x)) $, 即子层 $ (x) 是子层本身执行的函数 。 为了方便这些剩余连接, 模型中的所有子层以及嵌入层都产生维度$d{ mathr{ model=512美元的输出值 。


解码器 : 解码器由一堆6美元相同的层组成。 除每个编码器层的两个子层外, 解码器插入第三个子层, 使编码器堆的输出产生多头注意。 与编码器类似, 我们使用每个子层周围的剩余连接, 其次是层正常化。 我们还修改解码器堆堆中的自省子层, 以防止各位置关注随后的位置 。 这种掩码器加上输出嵌入被一个位置抵消的事实, 保证对 $ 位的预测只能取决于低于 $ 的位子上已知的产出 。


3.2 注意


关注函数可描述为对输出绘制查询和一组关键值对对等输出的绘图,其中查询、键、值和输出是所有矢量。输出是按加权和计算的。





图2 (左) 缩放点- 产品注意。 (右) 多负责人注意由平行运行的若干关注层组成。


的数值,其中每个数值的加权由查询与相应键的兼容性函数计算。


3.2.1 分点-产品注意


我们要求我们特别注意“标准点-产品注意”(图2),投入包括维度的查询和关键值$dk}$,维度的and值$dv}$。我们用所有键计算查询的点产品,将每个值除以$sqrt{dk}$,并应用软负负函数来获取值的权重 。


在实际操作中,我们同时计算一组查询的注意功能,同时将注意功能汇总到一个矩阵中,然后将当值和数值汇总到矩阵中,然后将当值和数值汇总到一个矩阵中,然后将当值和数值汇总到一个矩阵中,然后将当值和数值汇总到一个矩阵中,然后将当值和数值汇总到一个矩阵中,然后将当值和数值汇总到一个矩阵中。


$$\ mathrm{ 注意} (Q, K, V)\ mathrm{ 软max} (\ frac} K @ T @ sqrt{ dk} V)$$


最常用的注意功能是添加注意 [2] 和点产品(倍增) 注意。 点产品注意与我们的算法完全相同, 但缩放因子是 $\ textstyl_frac{ 1unsqrt{dkç$。 添加注意用一个隐藏层的进料- 前进网络来计算兼容性功能。 虽然两者在理论复杂性方面相似, 但点产品注意在实际中要快得多, 空间效率也更高, 因为可以用高度优化的矩阵乘法来实施 。


虽然对于小值的美元(k)值来说,两种机制的作用相似,但添加式注意效果优于点值产品注意,而没有增加大值的美元(3),我们怀疑对于大值的美元(k)值而言,点产品数量大增,将软负函数推向其梯度极小的区域。 4 为了抵消这种影响,我们将点值产品缩放为$(textstyle_frac{1unsqrt{dk) 美元。


3.2.2 多方关注


我们发现,与其用$dmathrm{model{$ - 维维键、值和查询来执行单一关注功能,不如用不同的、已学过线性预测的维度,直线性计算出查询、键和值(以美元计)、键和值(以美元计)、值(以美元计)、值(以美元计)、值(以美元计)、值(以美元计)、值(以美元计)、值(以美元计)、值(以美元计)、值(以美元计)、值(以美元计)、值(以美元计)、值(以美元计)和查询(以美元计)、值(以美元计)、值(以美元计)、值(以美元计)、值(美元计)、值(以美元计)、美元计(美元计)、(美元)、(美元)、(美元)和(美元)等值(以美元计)的单一关注功能,不如用单项计算出查询、键和(美元)- 维输出值(以线性计算出(美元)- 美元),我们发现,这有利于用直线式的方式将查询(以不同的、键和以不同的、美元计数计算出查询、键和数值计算出查询、键和值(美元)进行线性计算出查询、键、键和值(以美元)进行线式计算,从而得出(以不同的计算出(以美元)进行线性计算出(以不同的计算出)的查询,从而产生最后值(以得出最后值,从而得出最后值(如图2图所示最后值(以图),结果,如图2所示最后值(以图2所示),从而得出最后值)。


多头关注使得该模型能够联合关注不同职位上不同代表分空间的信息,只要有一个单独的关注头,平均抑制了这一点。


$$\ begin{ array} r lmathrm{ 摩尔提指揮官} (Q, K, V)\ mathrm{ 康卡特} (\ mathrm{ 頭){1},...,\ mathrm{头}{\fn方正黑体简体\fs18\b1\bord1\shad1\3cH2F2F2F}世界末日\ mathrm{ i\ materm{ 注意} (QW) (QW)~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~$$


其中的预测是参数矩阵 $Wiin\mathbb{Rdmathrm{modeltimes dk$, W iKRdmode{dk, WiV Rdmode}dv 和 $WOin\mathb{Rh dvtimes dmathrm{ model}$。


在这项工作中,我们使用8美元的平行关注层或头。对于其中的每一个层,我们使用$dkdddddmathrm{model}}/h\stackrel\cdot64$。由于每个头的尺寸降低,计算总成本与完全维度的单头关注相近。


3.2.3 在我们的模型中应用 " 注意 "


变形器以三种不同的方式利用多头目的关注:


• 在“编码器-编码器注意”层中,查询来自以前的解码器层,记忆键和值来自编码器的输出,这使编码器中的每一个位置都能在输入序列中的所有位置上参与。这模仿了典型的编码器-编码器注意机制,如[38、2、9] 序列顺序模型。• 编码器包含自读层,在自读层中,所有钥匙、值和查询都来自同一地点,在此情况下,编码器上一层的输出。编码器中的每个位置都可处理编码器前一层的所有位置。• 同样,解码器中的自我注意层使解码器中的每个位置都能在解码器中担任所有职位,直至并包括该位置。我们需要防止解码器中的左向信息流动,以维护自动递减属性。我们通过掩盖与非法连接相对应的软体输入中的所有价值(单位:$-infty),在扩大的点产品注意范围内执行这一规定。


3.3 定位向前种子网络


除了注意子层外,我们的编码器和解码器中的每个层都包含一个完全连接的进化前向网络,对每个位置分别和同样应用。这包括两个线性变换,在两个位置之间有ReLU的激活。


$$\ mathrm{ FFN}(x) {(x) {(x) {(x) {(x) {(x) {(x) } {(x) {(x) 1} {(b) {(1) } w} {(2) {(b) }}$$


虽然线性变换在不同位置上是相同的, 它们使用不同的参数, 从层到层。 另一种描述方式是两个内核大小的变异。 输入和输出的维度是 $dmathrm{ model =, = 512$, 而内层的维度是 $df f2048$ 。


3.4 嵌入和软性


与其他序列转换模型相似, 我们使用学习的嵌入式将输入符号和输出符号转换为维度的矢量 $dmathrm{ model=$。 我们还使用通常的学习线性变换和软负函数将解码输出转换为预测的下方概率。 在我们的模型中, 我们使用两个嵌入层和前方的“ 最大线性变” 之间相同的权重矩阵, 类似 [30] 。 在嵌入层中, 我们将这些重量乘以$\sqrt{ dmathrm{ model{ $ 。


表1:不同层型的最大路径长度、每层复杂程度和最低连续操作次数。 美元是序列长度,美元是代表的维度,美元是革命的内核大小,美元是受限制的自我注意的邻里大小。





3.5 定位编码


由于我们的模型没有重复,也没有变迁, 为了让模型使用序列顺序的顺序, 我们必须输入一些关于序列中符号的相对位置或绝对位置的信息。 为此, 我们在编码器和解码器堆的底部输入嵌入中添加“ 位置编码 ” 。 位置编码与嵌入值具有相同的维度 $dmathrm{ model$, 以便可以将两者相加。 位置编码有许多选择, 学习和固定 [9] 。


在这项工作中,我们使用不同频率的正弦和联弦功能:


$$\ begin{ rary}rP E( p o s s, 2i)  i n( p o s/ 100002i/ dmathrm{ model} {P E( p o s, 2i+1) c o s ( p o s/ 100002i/ dmathrm{ model}}\\ end{ray}$$


位置编码的每个维度都对应一个正弦值。波长形成一个几何级程,从 2\ pi 美元到 1 000\ cdot2\ pi 美元。我们之所以选择这个函数,是因为我们假设它能够让模型很容易地学会以相对位置参与,因为对于固定的折合美元来说, $P Ep o s+k} 美元可以作为PEpos 的线性函数。


我们还试验了使用所学的定位嵌入[9],发现这两个版本产生的结果几乎相同(见表3行(E))。 我们选择了类象型版本,因为它可能允许模型推断出序列长度比培训期间的长度长。


4 为什么自我注意


在本节中,我们比较了自我注意层的各个方面,与通常用于绘制符号表示值$(x1},......,xn})的可变长度序列的重复式和变动层的不同方面,与相等长度$(z1},...,...,zn}美元的另一个序列,与$x},zi},zi},in,\mathbf\bar{Rd}美元,例如典型序列中隐藏的转换编码器或解码器的层。


一个是每层的总计算复杂性。另一个是可平行计算的数量,用所需连续作业的最低数量来衡量。


第三是网络中长距离依赖之间的路径长度。 学习长距离依赖是许多顺序转换任务中的一个关键挑战。 影响了解这种依赖能力的一个关键因素是前方路径长度和后方信号在网络中必须穿行。 这些输入和输出序列中位置组合之间的路径越短,就越容易了解长距离依赖[12]。 因此,我们也比较了由不同层次类型组成的网络中任何两个输入和输出位置的最大路径长度。


如表1所示, " 自留层 " 将所有职位与按顺序执行的作业数量固定在一起,而 " 常列层 " 则需要连续作业(n)美元)。就计算的复杂性而言,当序列长度小于 " 美元 " 表示维度时,自留层比 " 常列层 " 更快,因为 " 美元 " 的顺序长度小于 " 代表维度 " 美元,而最常见的情况是,最先进的机器翻译模型,如单词 [38] 和 " 字节 " [31] 表示法,为了改进涉及非常长序列的任务的计算性能, " 自留 " 自我留 " 可限于只考虑以各自产出位置为中心输入序列的某个大小的 " 美元 " 区域 " 。这将将最大路径长度提高到 " 美元 " 美元 " /美元 " 。我们计划在今后的工作中进一步调查这一方法。


一个带有内核宽度为k<n$的单一革命层并不连接所有输入和输出位置的对齐。 如果是毗连的内核,则需要堆积值为O(n/k)美元(n)美元(n)美元(o(l o(g)k})美元(n)美元(l)美元(l),从而增加网络中任何两个位置之间最长路径的长度[18]。 革命层通常比经常层贵,以美元为单位。 然而,如果是相连的内核,则需要堆积值为O(n/k)美元(n)美元(n/k)美元(n)(n)美元(n),但是,即使以美元(l)为单位,分解的共振动过程的复杂性也相当于一个自留层和一个点向上方的供料层的组合,这是我们在模型中采用的方法。


作为旁观者,自我关注可以产生更多的可解释模型。 我们检查模型的注意力分布,并在附录中介绍和讨论实例。 不仅个人注意力主管明确学会执行不同的任务,而且许多人似乎表现出与句子的合成和语义结构有关的行为。


5 培训培训


本节介绍我们的模型的培训制度。


5.1 培训数据和汇总


我们接受了2014 WMT 英文-德文标准数据集培训,该数据集包括大约450万对判刑;判决用字节编码编码[3],该词有大约37 000个符号的共享源目标词汇;对于英文-法文,我们使用了大得多的WMT 2014 英文-法文数据集,包括36M 个句子和3 000个单词单词 [38] ;判刑对子按大约顺序排列;每批培训包括一套包含大约25 000个源符号和25 000个目标符号的判刑配对。


5.2 硬件和时间表


我们用8台NVIDIA P100 GPUs在一台机器上培训我们的模型。对于使用整个论文中所描述的超参数的我们的基模型,每个培训步骤花费了大约0.4秒。我们用总共100,000个步骤或12小时来培训基准模型。对于我们的大模型(在表3底线上描述),步骤时间是1.0秒。大模型用300,000个步骤(3.5天)来培训。


5.3 优化


我们用亚当优化器[20], 以$\beta10.9美元, $\beta20.98美元和$\epsilon=109美元。


$$l r t =\\ materhrm{ 模型} -05\ cdot\ operatorname\ min} (t e p_n u m- 0.5}, s t e p_n u m\ cdot w a r u p_s t e p sp s - 1.5}$$


这相当于在最初的热步骤培训步骤中以线性方式提高学习率,然后按比例降低到步数的反平方根。我们使用了 r um u p_s t e p s= 40000$。


5.4 规范化


在培训期间,我们采用三种正规化:


表2:变换器获得的BLEU分数比以往英文对德文和英文对法文新闻测试的最新模型要高,比培训费用低一分。





在加入次层输入和正常化之前,我们在每个子层的输出中应用辍学 [33] 。此外,我们使用在编码器和解码器堆叠中的嵌入数和位置编码总数来应用辍学。对于基本模型,我们使用$Pd r o p0.1$的速率。


培训期间,我们使用“平滑”标签,标注价值为$\epsilonl s0.1$[36]。这伤害了不解性,因为模型学会了更加不确定,但提高了准确性和BLEU分数。


6 结果6 结果


6.1 机器翻译


在WMT 2014 英文对德文翻译任务方面,大型变压器模型(表2中的变压器(大))比以前报告的最佳模型(包括组合)高出2.0倍以上的BLEU,建立了28.4的新的最新BLEU分数,该模型的配置列于表3的底线:培训用了3.5天,共8个P100GPU,甚至我们的基模型也超过了以前公布的所有模型和组合,比任何竞争性模型的培训费用少了一小部分。


在WMT 2014 英文对法文翻译任务方面,我们的大模型取得了41.0的BLEU分数,比以前出版的所有单一模型都高,比以前最先进的模型培训成本低1/4美元。 变异(大)模型为英语对法语使用的辍学率培训了0.3美元,而不是0.3美元。


对于基准模型,我们使用一个单一模型,平均使用最后5个检查站,每10分钟写一次;对于大模型,我们平均使用最后20个检查站;我们使用波束搜索,其面积为4和长度罚款为0.6美元[38]。这些超光谱仪是在对开发集进行试验后选择的。我们设定了输入长度为+50美元时的最大输出长度,但尽可能提前终止 [38] 。


表2总结了我们的成果,并将翻译质量和培训成本与文献中的其他模型结构进行了比较。我们通过乘以培训时间、所用GPU数量以及每个GPU 5 的持续单精度浮点容量估计,估算了用于培训模型的浮动点操作数量。


6.2 模型变化


为了评价变换器不同组成部分的重要性,我们以不同方式改变我们的基础模式,衡量在翻译中英文对德文翻译的绩效变化。


表3:变换器结构的变化情况:未列出的数值与基本模型相同。所有指标都载于英文对德翻译开发集,即2013年新闻测试。根据我们的字节和字节编码,列表中的不统一之处是每个字的字母,不应与每个字的不统一之处相比较。





我们使用了前一节所述的光束搜索,但没有平均检查站。我们在表3中列出了这些结果。


如第3.2.2.节所述,表3行(A)中,我们不同的是关注负责人和关注关键和价值层面的数量,使计算量保持不变,而单头的关注程度比最佳环境差0.9 BLEU,但质量也下降,领导人数过多。


在表3行(B)中,我们观察到减少注意关键键的大小$dk}美元会损害模型质量。这意味着确定兼容性并非易事,比点产品更复杂的兼容功能可能是有益的。我们在(C)和(D)行中进一步观察到,如预期,更大的模型更好,而退出对于避免过度滑动非常有帮助。在(E)行中,我们用学习的定位嵌入[9]来取代我们的正弦定位编码,并观察基本模型几乎相同的结果。


6.3 英国选区


为了评估变换器能否概括到我们在英语选区划分实验中完成的其他任务。 这项任务提出了具体的挑战:产出受到强大的结构性制约,而且远远长于输入时间。 此外,RNN 序列顺序模型未能在小数据系统中取得最新结果 [37] 。


我们在Penn Treebank [25] 华尔街日报部分用美元(WSJ)培训了一个四层变压器,大约40K个训练句,我们还在半监督环境下培训了一台四层变压器,使用较大的高信任度和BerkleyParker Corbora, 大约17M 句 [37] 。我们用了一个16K符号的词汇来设置WSJ, 并用一个32K符号的词汇来设置半监督环境。


我们仅进行了少量实验,以选择辍学者,包括注意力和剩余(第5.4节)、学习率和22节开发集的梁体大小,所有其他参数与英文对德基本翻译模式没有变化,在推断中,我们将最大产出长度提高到输入长度+;300美元。我们只用21和0.3美元的梁体大小,用于WSJ和半监督环境。


表4: " 变换器 " 向英文选区评分概括良好(结果载于 " 社会公正 " 第23节)





我们在表4中的结果显示,尽管没有根据具体任务调整我们的模型,但效果令人惊讶,除了经常性神经网络语法外,结果优于以往报告的所有模型[8]。


与RNN 序列到序列模型[37] 不同,变压器的性能超过了伯克利Parker[29],即使仅就WSJ40K句的一组培训培训进行了培训。


7 结论7 结论


在这项工作中,我们展示了 " 变换器 ",这是完全以注意力为基础的第一个序列转换模型,用多头自省取代了编码器-解码器结构中最常用的重复层。


在翻译任务方面,变换器的训练速度可以大大超过基于经常性或进化层的建筑。 在WMT 2014 英文对德文翻译任务和WMT 2014 英文对法文翻译任务中,我们实现了新水平。 在前一项任务中,我们最好的模型甚至超越了以前报告的组合。


我们对关注模式的未来感到兴奋,并计划将其应用于其他任务,我们计划将变异器扩大到涉及文本以外的投入和产出方式的问题,并调查本地的有限关注机制,以便有效地处理大量投入和产出,如图像、音频和视频。 降低代代相传程度是我们的另一个研究目标。


我们用来培训和评估模型的代码可在https://github.com/ hoorflow/tensor2tensor查阅。


我们感谢Nal Kalchbrenner和Stephan Gouws的富有成果的评论、纠正和启发。


参考参考资料


[1] Jimmy Lei Ba、Jamie Ryan Kiros和Geoffrey E Hinton,《图层正常化》。[2] Dzmitry Bahdanau、Kyunghyun Cho和Yoshua Bengio,通过联合学习校对和翻译,翻译神经机器。[3] Denny Britz, Anna Goldie, Minh-Thang Luong, 和Quoc V. Le. 大规模探索神经机器翻译结构,CORR, abs/1703.03906, 2017年。[4] Jianpeng Cheng、Li Dong和Mirella Lapata,用于机器阅读的长期短期记忆网络。[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk 和 Yoshua Bengio, 统计机翻译使用伦编码器-编码器的学习短语表示,CORR, abs/1406.1078, 2014年。[6] Francois Chollet. Xception: 深层学习与深度分离的演化。 arXiv print arxiv:161.02357, 2016年。[7] Jungyoung Chung、Caglar Gülçehre、Kyunghyun Cho和Yoshua Bengio,关于序列建模的封闭式经常性神经网络的经验评估,CorR, abs/1412.3555, 2014。 [8] Chris Dyer、Adhiguna Kuncoro、Miguel Ballesteros和Noah A. Smith, Renah A. Smith, 神经网络语法,NAACL 2016年proc. [9] Jonas Gehring、Michael Auli、David Grangier、Denis Yarats和Yann N. Dauphin, 序列学习的连续顺序。[10] Alex Graves. 与经常性神经网络生成序列。 arXiv print arxiv:1308.0850, 2013。[11] Kaiming He、Yangyu Zhang、Shaoqing Ren和Jian Sun。关于图像识别的深层残余学习。在IEEE计算机愿景和模式识别会议的会议记录中,2016年,第770-778页。[12] Sepp Hoshreiter、Yoshua Bengio、Paulo Frasconi和Jürgen Schmidhuber,经常网中的逐渐流动:学习长期依赖性的困难,2001年。[13] Sepp Hoshreiter和Jürgen Schmidhuber。长期短期内存。神经计算,9(8):1735-1780,1997。[14] Zhongqiang Huang 和 Mary Harper. 自我培训PCFG语法,并有各种语言的潜在说明。载于2009年自然语言处理经验方法会议记录,第832-841页,ACL,2009年8月。[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schhuster, Noam Shazeer 和 Yonghui Wu, 探索语言模型的局限性。 ArXiv print arXiv:1602.02410, 2016年。[16] Zukasz Kaiser和Samy Bengio,积极记忆能取代注意力吗?《神经信息处理系统的进展》,(NIPS),2016年。[17] Zukasz Kaiser和Ilya Sutskever。神经GPUs学习算法。在国际学习代表大会(ICLR),2016年。[18] Nal Kalchbrenner、Lasse Espeholt、Karen Simonyan、Aaron van den Oords、Alex Graves和Koray Kavukcuoglu. 线性时间神经机器翻译。 ARXiv print arXiv:1610. 10099v2, 2017年。[19] Yoon Kim、Carl Denton、Luong Hoang和Alexander M.Rush,结构化关注网络。在2017年国际学习代表大会上。[20] Diederik Kingma 和 Jimmy Ba. Adam: 一种施压优化方法。[21] Oleksii Kuchaiev和Boris Ginsburg,LSTM网络的集权技巧。 arXiv print arxiv:1703.10722, 2017年。[22] Zhouhan Lin、Minwei Feng、Cicero Nogueira dos Santos、Mo Yu、Bing Chang、Bowen Zhou和Yoshua Bengio,一个结构化的自我防范刑罚嵌入。[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, 和 Lukasz Kaiser, 学习序列的多重任务序列。 arXiv 预印 arXiv: 1511.06114, 2015 。[24] Minh-thang Luong, Hieu Pham和Christopher D Manning,《关注神经机器翻译的有效方法》,ARXiv预印:1508.0425,2015年。[25] Mitchell P Marcus、Mary Ann Marcinkiewicz 和 Beatrice Santorrini 。 建造一大批附有注释的英文文集:本树库。 计算语言学,19(2):313-330,1993年。[26] David McClosky、Eugene Charniak和Mark Johnson,《有效自我分析培训》,NAACL人类语言技术会议记录,主要会议,第152-159页,ACL,2006年6月。[27] Ankur Parikh、Oscar Täckström、Dipanjan Das和Jakob Uszkoreit,一个分解关注模式,《自然语言处理的经验方法》,2016年。[28] Romain Paulus、Caiming Xiong和Richard Socher,一个深为强化的抽象总结模型。ArXiv print arXiv:1705.04304, 2017。[29] Slav Petrov, Leon Barrett, Romain Thibaux, 和 Dan Klein, 学习准确、紧凑和可解释的树说明。 载于第二十一届计算语言国际会议记录和ACL第44届年会记录,第433-440页。 ACL, 2006年7月。[30] Ofir Press和Lior Wolf。利用输出嵌入来改进语言模式。 arXiv print arxiv:1608.055859, 2016年。[31] Rico Sennrich、Barry Haddow和Alexandra Birch. 神经机器翻译有子词单位的稀有词。[32] Noam Shazeer、Azalia Mirhoseini、Krzysztof Maziarz、Andy Davis、Quoc Le、Geoffrey Hinton和Jeff Dean. 异常庞大的神经网络:专家混合层稀疏。[33] Nitish Srivastava、Geoffrey E Hinton、Alex Krizhevsky、Ilya Sutskever和Ruslan Salakhutdinov。辍学:防止神经网络过度安装的简单方法。 《机器学习研究杂志》,15(1):1929-1958,2014年。[34] Sainbayar Sukhbaatar、Arthur Szlam、Jason Weston和Rob Fergus, 端对端记忆网络,载于C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama和R. Garnett, 编辑,《神经信息处理系统的进展》,第28页,第2440-2448页。 Curran Associates, Inc., 2015年。[35] Ilya Sutskever, Oriol Vinyals, 和 Quoc VV Le. 与神经网络进行序列学习的顺序。《神经信息处理系统的进步》,第3104-3112页,2014年。[36] Christian Szegedy、Vincent Vanhoucke、Sergey Ioffe、Jonathon Shlens和Zbigniew Wojna. 重新思考计算机视觉的初始架构,CORR, abs/1512.00567,2015年。[37] Vinyals & Kaiser、Koo、Petrov、Sutskever和Hinton语作为外语。《神经信息处理系统的进展》,2015年。[38] Yonghui Wu, Mike Schhuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, 元曹, Qin Gao, Klaus Macherey等人, Google的神经机器翻译系统:弥合人与机器翻译之间的差距。 ArXiv 预印:1609.08144, 2016年。[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, 和 Wei Xu. 具有神经机翻译快速前向连接的深层重复式模型,CorR, abs/1606.04199, 2016年。[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. 快速和准确的转换-降低成分剖析。 ACL第51届年会记录(第1卷:长篇),第434-443页。 ACL, 2013年8月。


关注视觉





图3:第6层第5层的编码器自我注意的远距离依赖性之后的注意机制实例6:许多注意负责人注意动词`制造'的遥远依赖性,填写了`制造.更困难'的短语。这里只对`制造'一词表示注意。不同的颜色代表着不同的头目。最好的用颜色来看待。





图 4: 关注对象的两位头目,同样在第6层第5层中,明显地参与了厌食动物决议。顶层:关注对象充分。顶层:关注对象充分。底层:关注对象仅来自“它”这个词,关注对象为第5和第6层。请注意,关注对象对于这个词来说是非常尖锐的。


图5:许多受关注的负责人表现出似乎与该句的结构有关的行为,我们从6层第5层编码员自我注意的两个不同的负责人的上面举了两个这样的例子,这些负责人显然学会执行不同的任务。
